# CIPGAN

### Introduction

> ç”» ç‹å®‰çŸ³
>
> è¿œçœ‹å±±æœ‰è‰²ï¼Œè¿‘å¬æ°´æ— å£°ã€‚
>
> æ˜¥å»èŠ±è¿˜åœ¨ï¼Œäººæ¥é¸Ÿä¸æƒŠã€‚

ä¸€å¼ ç”»çº¸ï¼Œç”»å¾—æ»¡æ»¡å½“å½“ä¸ç•™ä¸€ç‚¹å„¿ç©ºç™½ï¼Œæ˜¯è¥¿æ´‹æ²¹å½©ç”»ã€‚ä¸€å¼ ç”»çº¸ï¼Œå¯¥å¯¥æ•°ç¬”ä¸¹é’äºç™½å®£ä¹‹ä¸Šï¼Œæ˜¯ä¸­å›½ç”»ã€‚æ¢µé«˜æµ“å¢¨é‡å½©æˆä¸ºæƒŠè‰³ä¼ ä¸–ä¹‹ä½œï¼Œé½ç™½çŸ³æ°´å¢¨ä¸¹é’ä¹Ÿæ˜¯ç”»ç•Œä¼ å¥‡ã€‚

å›½ç”»ä¹ƒæˆ‘å›½ä¼ ç»Ÿæ–‡åŒ–è‰ºæœ¯çš„ç‘°å®ï¼Œè•´å«ç€ä¸­å›½äººçš„æ–‡åŒ–ä¸æ™ºæ…§ã€‚éšç€æˆ‘å›½æ–‡åŒ–å½±å“åŠ›çš„æé«˜ï¼Œå›½ç”»è‰ºæœ¯æ›´æ˜¯å¤‡å—å…³æ³¨ã€‚

æœ¬é¡¹ç›®ä¸ºåŸºäºCycleGANçš„ä¸­å›½ç”»é£æ ¼è¿ç§»æ¨¡å‹ã€‚ä¸å€Ÿä¸ç¬”å¢¨çº¸ç šï¼Œä¸æ‹˜äºä¸€æ–¹ç”»å¸ƒï¼Œä¹Ÿä¸éœ€æœ‰ä¸“ä¸šçš„ç»˜ç”»åŠŸåº•ï¼Œå€Ÿè®¡ç®—æœºè§†è§‰ä¹‹æ‰‹ï¼Œå³å¯å°†ç…§ç‰‡è½¬æ¢ä¸ºæ°´å¢¨ç”»é£æ ¼ï¼Œåˆ›ä½œå‡ºè‡ªå·±çš„ä½œå“ã€‚



### Getting Started
- Clone this repo:
```bash
git clone https://github.com/Wenretium/CIPGAN
cd CIPGAN
```

- type the command `pip install -r requirements.txt`.

- You may meet some errors when following the provided file requirements.txt. Actually most of the default downloading versions of pytorch-gpu is available. Here's my environment and hope it helps.  (The pretrained models provided are trained on Ubuntu system.)

  ```
  # Windows 10
  python==3.7
  torch==1.8.1+cu102
  torchvision==0.9.1+cu102
  dominate==2.6.0
  visdom==0.1.8
  opencv-python==4.5.2.54
  numpy==1.20.3
  pillow==8.2.0
  
  # Ubuntu 16.04
  python==3.6
  pytorch==1.4.0+cu101
  torchvision==0.5.0
  dominate==2.4.0
  visdom==0.1.8
  opencv-python==4.5.2.54
  numpy==1.16.4
  pillow==6.1.0
  ```

  

### Train
#### 1. Prepare a dataset including two different domains, and organize it as follows. (The same as CycleGAN)

```
CIP_dataset1_process
--trainA
--trainB
--testA
--testB
```
I also provide my own training dataset, including `CIP_dataset1_process` and `CIP_dataset2`. Theyâ€™re collected by me from the Internet (sorry to forget the specific source). You can download it from [here](https://pan.baidu.com/s/1sWQwcun-lq0YEc-0WQRXAg) (203n).

#### 2. Train a model

+ original
```
python train.py --dataroot ./datasets/CIP_dataset1_process --name CIP_dataset1 
```
+ +vgg
```
python train.py --dataroot ./datasets/CIP_dataset1_process --name CIP_dataset1 --vgg
```
+ +vgg and attention (thresh is set to 0.8 by default)
```
python train.py --dataroot ./datasets/CIP_dataset1_process --name CIP_dataset1 --vgg --self_attention
```
+ To view training results and loss plots, run python -m visdom.server and click the URL http://localhost:8097.



### Test
+ without self-attention
```
python test.py --dataroot ./datasets/CIP_dataset1_process --name CIP_dataset1 
```
+ with self-attention (change `self_attention_thresh` here)
```
python test.py --dataroot ./datasets/CIP_dataset1_process --name CIP_dataset1 --self_attention --self_attention_thresh 0.8
```
If you don't want to resize the images when testing, add `--preprocess none` to the end.

The test results will be saved to a html file here: `./testresults/CIP_dataset1/latest_test/index.html`.



### Demo
We provide a test demo with a simple GUI. This part is located in  `./demo_files` and we provide 4 pretrained models for ablation study.

Run `./gui.py` and interact with our demo.

![1](README/demo.png)

You can choose your own test image and get the result immediately. The results are saved in  `./demo_files/results`.



### Architecture

#### Overview

![model](README/model.png)

<img src="README/model2.png">


#### Contributes

We choose the famous unsupervised style transfer model CycleGAN as our baseline. To attain  a better visual performance,  we propose two improvements:

+ In addition to the original loss function,  we add a new VGG loss. We use a pretrained VGG16 model from EnlightenGAN to obtain feature information of the input image and the corresponding output image, and set it as a loss component to restrain the variation of them. By this means, we hope to retain the semantic informantion of the input image as much as possible.

+ When training and testing on the original networks design of CycleGAN, we notice that there are artifacts in different degrees in our results. They usually appear in the sky, in where colors are often light and have an uneven distribution. To restrain the appearance of artifacts, we apply self-attention to our networks.  

  Before the images are put into the generator, we get their maps at first. We simply use the pixel values of an image to evaluate its intensity. In Chinese ink paintings, painters prefer to apply the ink on the dark regions of a landscape, such as the dark region and the edges of a mountain. So, we suppose that an intensity map is helpful to attach the attention to the proper regions of an image, and thus provides a more authentic result.

  In addition, we provide a control factor `self_attention_thresh` . It controls the values of the self-attention map. When the value of it decreases, that means we pay less attention to the other light-color details of the image.  



### Our project

#### Poster

![poster](README/poster.png)

#### Supplemental material

![supplemental material](README/supplemental_material.png)



### Updating

ğŸ“Œ March 27th, 2022: Update the source link of my dataset.

ğŸ“Œ April 12th, 2022: Update the formula figure and correct some typos.
